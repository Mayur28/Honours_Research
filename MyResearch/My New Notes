THOROUGHLY UNDERSTAND THE SWITCHING OF LABELS IN backward_G

SPICE EVERYTHING UP, TOO SIMILIAR!!!
Blend it with Mask Shadow GAN
 Find other papers as well
 Do with my own understanding!!!
This is just temporary, be ruthless thereafter

 Check the story about the transformation needed for preprocessing ( If RGB-> BGR is really necessary)
 I AM working directly in BGR, only when saving for display images, save the image in RGB

 Check if the config thing in EGAN is really necessary or not.

 # There is a lot of variation that can come out of the VGG functions
 Really make this my own! Experiment and only if really broken, revert to their altered form

 Once everything seems to be working, see if I can form the generator programmatically! #--> I bookmarked the solution

 A lot of the variation will also come from the generator's and discriminator's setup

 Between adjusting the dataset and removing the config, we're doing fewer 'iterations' per an epoch but this hasn't affected the quality of performance ( also reduced the time to 78-80 sec)

 Look at the alternate configurations of EGAN and experiment

 Go beyond what I wanted and state how exactly do I want to improve this in the long run

 Check if it is absolutely necessary to have skip connections in the Unet generator

 Check what does the super thing mean

 Try to understand how exactly to maneuver the input tensor (particularly in the get_target tensor function)
#Start with the above point tomorrow as it is pertinent!!!


 Try to find a way to plot the loss after every xxx epochs

 I particularly need to look into the get_target tensor function. There can be significant improvement

 The handling of the backward functions for the discriminator can be significantly improved!

 There is a conflict in using OpenCV ( one of them being unable to quickly convert the images to grayscale)


Go above and beyond of what they did... Be creative and include the graph of the loss...
Remove all the GPU id stuff... Its just unneccesary clutter and I will probably just work on Colab permanently


Fix the conversion functions ( TensorToImage,etc)... Right now, they are directly copied over!!!

New Notes from Setup Training function
#(Change the 'help' eventually!)
# vgg_choose will be set to relu5_1. Remove the if-statements in networks.py--> Check why was this option used
# Examine their multiple approaches again and try to combine uniquely
# They are using maxpooling in the generator, not avg_pooling... Check Radford's appraoch to downsampling
# I want tanh at the end of mine!--> Check if this would break the definition of an LSGAN
# Default setting doesn't use 'lighten' which normalizes the attention map... Experiment with this! Only appears just before
#the attention map calculation...
# Theres actually a lot that I removed from 'UnalignedDataset' that appears to relate to data augmentation... Experiment with this
# What does pool_size do and affect results?

What is the effect of the rate of convergence on the quality of the Output
# Focus on reducing noise!
 I have to thoroughly understand the difference between a normal GAN, using MSE loss and LSGAN( I am using an LSGAN)... Look into
 why this is a good choice!
 Check difference between Variable and Tensor (seems to have something to do with how gradients are calculated but surely tensors are also capable?)
Augment tensorboard!!!
I dont need to go all the way to 200 epochs
Overhaul the entire structure is I were starting from scratch
Check if data augmentation is really necessary when we have an abundance of unpaired data to train on!
Do something truly different!
Look into optimizing choice of batch size ( this seems very influential)... smaller batches= more significant changes per an update ( this is
more noisy, though, the noise can help with the model's generalization capabilities)

Check why is a 13x13 grid produced by the discriminator for each image in the batch
Make sure the configuration of the PatchGAN is perfect! ( Structurally)
Check what exactly is going on in backward_G
Try increasing the number of patches that we extract
Look into whether the order of training the generator and the discriminator is correct or not
What is ragan? It is used by the global disc when backpropping but not used by the local discriminator
Look into better ways of handling the backward functions (particularly the loss function stuff)
 Experiment what happens if I also adjust the disc loss where I do not subtract ( generator held up quite well)
 Why is preprocessing needed for the VGG network to convert from [RGB-->BGR] and [-1,1]-->[0,255]??? Could be with the way the vgg model was trained? There could be a discrepancy

 Check if vgg_mean is actually useful or not? Seems to have potential?
 Look into how vgg and vgg_loss are different... self.vgg_loss.compute_vgg_loss(self.vgg)... vgg_loss is a class and useful and uses the accepts self.vgg as input ... vgg is only the network, nothing else!
 Why was relu5_1 used for the VGG network?
