MAJOR OVERHAUL NEEDED!!!!

Dont just carry on peeling away, there needs to be substance that still remains! --> Gordon Murray!


I'm now starting a THOROUGH POLISH!!!!
ALL QUESTIONS POSSIBLE AND QUESTIONING EVERYTHING

- Make the setup automatic- Find A way to adaptively set the checkpoint directly without needing to set it ( this is only applicable when executed locally)


To Do
I Still need to be very scientific when I adjust certain parameters( such as changing from instance to batch)
    I tested between  instance and batch norm... They both take the same amount of time and in terms of quality of results , I cant see anything?... If anything, I would rather go for Batch since I think it has better stabilization properties
- Start looking into setting up the infrastructure for the testing environment
    Look into better ways of saving the model or fix the current version... I tried revamping it, made no change???Before making any further tries, check if my version is still usable or not!
- Try to get the network more efficient!!! 100 secs is unacceptable, although, dont compromise quality of results
- For vgg_choose, check what was the purpose of having so many different options.
- Where does the perceptual loss fit in?



Experiments
- Investigate the effect of decreasing the batch size
- vgg_choose will be set to relu5_1. Remove the if-statements in networks.py--> Check why was this option used
- Experiment with Normal Relu and Leaky Relu in the generator?
- Consider using batch_size=8... Dont be premature in the verdict... Read hardt carefully before making this a pacifier for everything...


Few Minutes
- Check if it is always the case that we set affine to False for InstanceNorm2d?
- (Change the 'help' eventually!) in Setup Training
- Remove the need for explicitly specifying the checkpoint directory... Just create at a certain level!


Reading Up Investigation
- Check again if I should decay the learning rate? What are others doing?--> Check isola and  others MaskShadowGAN
- Look into how Tensor and Variable works!
- Extract alot of variation from the VGG network
- If possible look into variation in the Perceptual Loss
- Look into what can be done to reduce noise
- The PatchGAN now has BatchNormalization (WITH BIASES, ONCE THE COLOR BIAS IS SORTED OUT, CHECK IF THIS NORM BIAS IS NECESSARY OR NOT)
- READ THE LATEST COLLECTIONS OF ARTICLES AS POSSIBLE  TO GAIN A DEEP UNDERSTANDING
- Try looking into instance noise which adds noise to the real and synthetic samples They used additive gaussian noise ... Variance needs to decrease over time
- Go through all Pytorch documentation to explore what is possible
- What does .data do?
- Look into the LATENT stuff for once and for all!

Concrete Investigative list
- The code seems to be very sensitive to the version of Pytorch used! The gray_transform was a mission (more particularly A-Img to Grayscale(with one channel))
- Investigate how was the low-light images of egan formed? They seem very artificial... An example being 23_2... The sky cant be that dim... In the paper, that mention the 4 sources (2 normal) and 2 HDR sources... INVESTIGATE THIS THOROUGHLY... could help with quality of performance.
- Experiment the effect when instance normalization is used
- vgg_choose will be set to relu5_1. Remove the if-statements in networks.py--> Check why was this option used
- I need to thoroughly understand the difference between a normal GAN, using MSE loss and LSGAN( I am using an LSGAN)... Look into why this is a good choice!
- Check why is a 13x13 grid produced by the discriminator for each image in the batch ( Dont doubt this) Get Mines in line with Isola
- Why was relu5_1 used for the VGG network?


FIND OUT FROM HAIRONG
I tested between  instance and batch norm... They both take the same amount of time and in terms of quality of results , I cant see anything?... If anything, I would rather go for Batch since I think it has better stabilization properties


Notes
- Point out the lies in EnlightenGAN's paper ( That they using Unet)... I cant point fingers for the normalization aspect... They set the norm to 'instance' but dont actually use it (it is only used by other generator architectures)
- Note that the default that they set uses Instance normalization in the generator!!! --> This has been verified
    ToDo: Try with batch normalization
- If I do get the speedup, make sure I compare the size of my network to Official EGAN!
- For comparison, compare to paired approaches and consider below option
    Look at FID for evaluating the results (Read the Pytorch implementation of CycleGan by Chinese Isola)
- It is now verified that it was the 0.0002 lr that caused the weird effects
- Ragan is the relativistic discriminator used for the global generator and discriminator only (this is where we are swapping the labels)
- From a performance perspective, I am capable of achieving about 76 sec epochs in the bare bones approach, although, performance should'nt be made the only priority... EGAN's default is 87 seconds... Mention the tradeoff of speed and quality
- Note the pattern for the number of filters used in the generator, when downsampling, the number of filters doubles. When upsampling, the number of filters is halved. Additionally, The number of filters is equal in mirrored layers (for example. the first layer has the sample number of filters as the last layer.)
- Very important, we do not perform normalization in the first layer of the generator. (needs to be accounted for seperately, the rest can be produced algorithmically)--> EGAN has an extra layer and does not use normalization which shouldn't primarily determine the performance of the entire algorithm.
- My understanding of EGAN is slightly flawed. I thought EGAN primarily uses instance normalization which isnt the case. Instead, it primarily uses batch norm but uses instance norm before the VGG for stability
- The discriminator is built algorithmically by defining a batch and using it as many times that is specified(n_layers_D)
- The dataloader loads the data into batches ( and pretty much handles all the data handling) by using the GPU. LOOK INTO THE DATALOADER STUFF THOROUGHLY!! To use the dataloader stuff, it is compulsory to implement the __getitem__ and __len__ magic functions
- The 'data' training loop is a dictionary where each element is a tensor (with dimensions 16x3xsizexsize). These tensors represent A,B,input_A_gray and input_imgs. It also stores 'A_paths' which is the path to the training images accessed in that batch. Go deeper into how this dictionary is actually formed using the data loader.
- Remember, single model is the grand network which contains the generator and the discriminator. (we are working with an instance of 'single model'). That is why in 'optimize_parameters', we only calling forward() once which will propagate through the generator and the discriminator.  Note that we are doing the alternating training batches thing as mentioned by Radford.
- The __getitems__ function in unaligned_dataset is called each iteration and is used to form the dictionary form of the batch with the A,B,input image and attention map!
I changed DataParallel (towards the end of the definition of the generator and the discriminator) because at the moment, I'm only using one GPU.This function basically chunks the input across all the GPU's (uncomment if Shun makes a plan)
- When they say that they concatenate and reshaping the attention maps to the filter size, we actually do this in the decoder( get multiplied when we are upsampling)
- When I want to see the latent stuff, the attention map and the patchs, uncomment the original dictionary setting.
- It appears that the output is formed by multiplying the latent image with the low-light image.
- Mask- Shadow GAN has it like original paper (opposite to EGAN) : They put Norm. before Relu????
- The LSGAN can be implemented by using the target values of 1.0 for real and 0.0 for fake images and optimizing the model using the mean squared error (MSE) loss function, e.g. L2 loss. The output layer of the discriminator model must be a linear activation function.
- Check if there is a resize function in single_model.py ( There isn't in the entire project. Note that looking at the base_dataset, it accepts torch.utils.data which is built-in and explicitly specifies that egt_im and len need to be overwritten). the 'resize_' function is a built-in tensor function.
- The type/ form of training data can have a huge impact on what the GAN achieves
- Find out the following from Richard: Find out the following: Its usually Conv->Batch-> Relu but here its Conv-> Relu->Batch. Answer! MLM states that this is not an issue. Org. paper has it like normal but experimentation revealed that better perf.

LSGAN notes
- The blog post is intuitive...
- The least squares loss will penalize generated images based on their distance from the decision boundary.
- The output layer of the discriminator model must be a linear activation function.
- Below is an example configuration from MLM: This involves the use of Convolution-BatchNorm-Activation layer blocks with the use of 2Ã—2 stride for downsampling and transpose convolutional layers for upsampling. LeakyReLU activation layers are used in the discriminator and ReLU activation layers are used in the generator.
- Using an L2 loss penalizes and forces the synthetic dist to match the actual distribution instead of just hoping that the discriminator classifies correctly. We work with the distance between the actually distribution and our sythetic distribution
- Using the original log loss isn't effective as the discs discriminator is saturating, and since the generator is updated on the discriminators performance, G may not have sufficient information to direct itself towards the true data distribution


Notes for the Report:
- When writing the report, explain the effect of varying every single parameter individually! ( If it seems right!)
- Talk about using the transpose convolution which gave checkerboard effects, therefore, I'm going to consider some form of interpolation then performing a convolution
- For the report, I should definitely include a diagram of the architecture since she specifically asked for it ( but it needs to be indistinguishable from Enlighten-GAN!)
- (Outcome pending on this) In my report, I should show off that I dont need anything too dramatic for shadow detection (like ST-CGAN that dedicates an entire GAN for it)
- (Outcome pending) Find a way to do erosion and dilation to the attention map. Try doing these operations before adding to tensor but the noise removal will probably be expensive.
- When writing the report, describe how the data is pre-processed!
- In my write-up make sure I mention that Im using LSGAN loss and explain it. In the discriminator, instead of just being right or wrong, it tells how right or wrong we are.
- Find a way to remove the noise
- Be absolutely precise when it comes to detail, for example, when discussing the dataset, go into detail of where the images came from, what do they contain, what is it's form(png and size), why did I use it... Were they preprocessed
Make a note that I made this change : Check what is the best way to downsample--> EGAN: MaxPool.... I'm following Radford and doing fractionally strided convolution ( Give a thorough explain why this is the case)
- The generator and discriminator are DCGAN's but the grand network is considered as a LSGAN (the name primarily originating from the choice of loss function)
- In the paper, focus thoroughly on the significance of the attention map
- Also explain the trajectory, how if first ended up with the UNet_resize_conv and reverted... Try to display some of the results
- When writing the paper, revise again the theory behind an LSGAN (she obviously doesn't know what it is--> Change that!)
