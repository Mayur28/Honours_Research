MAJOR OVERHAUL NEEDED!!!!

Compare between "Remove individual patch" and "pinned memory"... Only the network.py is different ( Where I tried to optimize the generator and discriminator)
# To get a definitive answer, compare the results of optimized version( for 200 epoches) and my "modified" GAN that converges faster but trained for 150 epoches (15000 vs 12000 seconds)
The optimization improved the rate of convergence but worsened the execution runtime
Optimized timing: 80-82 secs, old version (that converges and mirrors their approach: 76 secs)

For any verification, execute "Minor changes to Perfect" from GitHub
It is now verified that it was the 0.0002 lr that caused the weird effects
Look into any potential Perceptual Loss enhancements!

Question the trade off between quality of results and runtime?
Its definitely my updated generator and discriminator thats causing an issue...


To Do Today
Change the generator to use skip connections!!! Do Not Lie!!!
Fine tune the attention map --> Today!
- Check if I can optimize how VGG can be used!
Check if eventually decaying the learning rate is really useful or not.
Look into other types of loss functions that I could use.
Sort out upsampling in Generator (remove there bilinear and conv layer nonsense) # Sort the conv. rate and runtime trade-off  --> Then only consider this
Try looking into instance noise which adds noise to the real and synthetic samples They used additive gaussian noise ... Variance needs to decrease over time


LSGAN notes
The blog post is intuitive...
the least squares loss will penalize generated images based on their distance from the decision boundary.
The output layer of the discriminator model must be a linear activation function.
Below is an example configuration from MLM: This involves the use of Convolution-BatchNorm-Activation layer blocks with the use of 2Ã—2 stride for downsampling and transpose convolutional layers for upsampling. LeakyReLU activation layers are used in the discriminator and ReLU activation layers are used in the generator.
Using an L2 loss penalizes and forces the synthetic dist to match the actual distribution instead of just hoping that the discriminator classifies correctly. We work with the distance between the actually distribution and our sythetic distribution
Using the original log loss isn't effective as the discs discriminator is saturating, and since the generator is updated on the discriminators performance, G may not have sufficient information to direct itself towards the true data distribution

 Find other papers as well
 Do with my own understanding!!!
The least square loss function is the L2 loss functions
MLM only mentions having a linear activation function in the output layer of the discriminator, therefore, I can add in the tanh to the generator's output

Concrete To Do List
- Check how can I manuovre the dataset to get the most out of it? In terms of pre-processing, size, etc?

Eventually, I need to ensure that it automatically creates the necessary directories
Go through all Pytorch documentation to explore what is possible
The training and testing execution path can be handled significantly better. Right now, they seem to be merged together somehow
Investigate the effect of increasing the number of patches
Experiment the effect when instance normalization is used
Consider using batch_size=8... Dont be premature in the verdict... Read hardt carefully before making this a pacifier for everything...
 Really make this my own! Experiment and only if really broken, revert to their altered form
 Once everything seems to be working, see if I can form the generator programmatically! #--> I bookmarked the solution ( OR Just create mini modules/ units!)
 Try to find a way to plot the loss after every xxx epochs ( Try to link it using tensorboard and try a dummy example on Colab before commiting)
 The handling of the backward functions for the discriminator can be significantly improved!
 Fix the conversion functions ( TensorToImage,etc)... Right now, they are directly copied over!!!
 #(Change the 'help' eventually!)
 # Examine their multiple approaches again and try to combine uniquely
 # Default setting doesn't use 'lighten' which normalizes the attention map... Experiment with this! Only appears just before
 #the attention map calculation...
 # Focus on reducing noise!
 Look into optimizing choice of batch size ( this seems very influential)... smaller batches= more significant changes per an update ( this is
 more noisy, though, the noise can help with the model's generalization capabilities)
Investigate the effect of increasing the number of patches that we extract
- I MUST BLEND WITH THE PyTorch DCGAN EXAMPLE AND CHECK THE OVERLAPS AND DISPARITIES
- Check what size images we are working with. The paper and dataset reflects 600x400 but the processing seems to account for 320x320
- In the long term, I may need to compile better datasets. I dont think training for the 2 roles simulataneously is a viable solution. FIND A BETTER STRATEGY TO TRAIN FOR THE 2 TASKS

- Look into the main functions (the arch. of the networks as well as how they forward and back propagate)
- It appears that the output is formed by multiplying the latent image with the low-light image. (Verify this!)
- Check what is the purpose of the latent result
- I dont see where is the vgg forward function called? In the Perceptual loss class, I see that we are using the vgg network but I dont see where we explicitly forward propagate. We dont, but investigate how is a result produced if we do not forward propagate




Concrete Investigative list
Investigate how was the low-light images of egan formed? They seem very artificial... An example being 23_2... The sky cant be that dim... In the paper, that mention the 4 sources (2 normal) and 2 HDR sources... INVESTIGATE THIS THOROUGHLY... could help with quality of performance.
When writing the report, explain the effect of varying every single parameter individually!
Experiment the effect when instance normalization is used
 Check the story about the transformation needed for preprocessing ( If RGB-> BGR is really necessary) # This relates to how the pre-trained model was trained
 Go beyond what I wanted and state how exactly do I want to improve this in the long run
 Check if it is absolutely necessary to have skip connections in the Unet generator
 Try to understand how exactly to maneuver the input tensor (particularly in the get_target tensor function)
 # vgg_choose will be set to relu5_1. Remove the if-statements in networks.py--> Check why was this option used
 What is the effect of the rate of convergence on the quality of the Output
 I need to thoroughly understand the difference between a normal GAN, using MSE loss and LSGAN( I am using an LSGAN)... Look into why this is a good choice!
 Check difference between Variable and Tensor (seems to have something to do with how gradients are calculated but surely tensors are also capable?)
 # Focus on reducing noise!
 Check why is a 13x13 grid produced by the discriminator for each image in the batch
Look into better ways of handling the backward functions (particularly the loss function stuff)--> Very messy
 Why is preprocessing needed for the VGG network to convert from [RGB-->BGR] and [-1,1]-->[0,255]??? Could be with the way the vgg model was trained? Correct!
 Check if vgg_mean is actually useful or not? Seems to have potential?
 Look into how vgg and vgg_loss are different... self.vgg_loss.compute_vgg_loss(self.vgg)... vgg_loss is a class and useful and uses the accepts self.vgg as input ... vgg is only the network, nothing else!
 Why was relu5_1 used for the VGG network?



Other notes
-Its interesting that GANs only generate square images!
THe PatchGAN structure is perfect!
Note! Increasing from EGAN 6 patches to 8 patches, improvement seems likely but increases the runtime by 4-5 seconds... Dont only focus on runtime and neglect quality of results.... I like the results when 8 patches are used despite the longer runtime... Mention this caveat in the report

From Radford
Replace Maxpooling with spatial convolutions to downsample which allows the network to learn its own spatial downsampling
apparently Above is used in the discriminator as well
Use batch normalization wherever possible, accept for the output layer of the generator and the input layer of the discriminator
For weight initialization, zero mean and std dev=0.02. Leak=0.2

Recent analysis has shown that there is a direct link between how fast models learn and their generalization performance (Hardt et al., 2015).
For above, dont get premature with initial results, wait it out.
Surprisingly, Mask shadow GAN obeys alot from Radford but uses Instance norm in both networks



Notes
- Ragan is the relativistic discriminator used for the global generator and discriminator only (this is where we are swapping the labels)
- From a performance perspective, I am capable of achieving about 76 sec epochs in the bare bones approach, although, performance should'nt be made the only priority... EGAN's default is 87 seconds... Mention the tradeoff of speed and quality
- Note the pattern for the number of filters used in the generator, when downsampling, the number of filters doubles. When upsampling, the number of filters is halved. Additionally, The number of filters is equal in mirrored layers (for example. the first layer has the sample number of filters as the last layer.)
- Very important, we do not perform normalization in the first layer of the generator. (needs to be accounted for seperately, the rest can be produced algorithmically)--> EGAN has an extra layer and does not use normalization which shouldn't primarily determine the performance of the entire algorithm.
- My understanding of EGAN is slightly flawed. I thought EGAN primarily uses instance normalization which isnt the case. Instead, it primarily uses batch norm but uses instance norm before the VGG for stability
- The discriminator is built algorithmically by defining a batch and using it as many times that is specified(n_layers_D)
- The dataloader loads the data into batches ( and pretty much handles all the data handling) by using the GPU. LOOK INTO THE DATALOADER STUFF THOROUGHLY!! To use the dataloader stuff, it is compulsory to implement the __getitem__ and __len__ magic functions
- The 'data' training loop is a dictionary where each element is a tensor (with dimensions 16x3xsizexsize). These tensors represent A,B,input_A_gray and input_imgs. It also stores 'A_paths' which is the path to the training images accessed in that batch. Go deeper into how this dictionary is actually formed using the data loader.
- Remember, single model is the grand network which contains the generator and the discriminator. (we are working with an instance of 'single model'). That is why in 'optimize_parameters', we only calling forward() once which will propagate through the generator and the discriminator.  Note that we are doing the alternating training batches thing as mentioned by Radford.
- The __getitems__ function in unaligned_dataset is called each iteration and is used to form the dictionary form of the batch with the A,B,input image and attention map!
I changed DataParallel (towards the end of the definition of the generator and the discriminator) because at the moment, I'm only using one GPU.This function basically chunks the input across all the GPU's (uncomment if Shun makes a plan)
- When they say that they concatenate and reshaping the attention maps to the filter size, we actually do this in the decoder( get multiplied when we are upsampling)
- When I want to see the latent stuff, the attention map and the patchs, uncomment the original dictionary setting.
- It appears that the output is formed by multiplying the latent image with the low-light image.
- Mask- Shadow GAN has it like original paper (opposite to EGAN) : They put Norm. before Relu????
- The LSGAN can be implemented by using the target values of 1.0 for real and 0.0 for fake images and optimizing the model using the mean squared error (MSE) loss function, e.g. L2 loss. The output layer of the discriminator model must be a linear activation function.
- Check if there is a resize function in single_model.py ( There isn't in the entire project. Note that looking at the base_dataset, it accepts torch.utils.data which is built-in and explicitly specifies that egt_im and len need to be overwritten). the 'resize_' function is a built-in tensor function.
- The type/ form of training data can have a huge impact on what the GAN achieves



Questions
- Where does the perceptual loss fit in?
- I see where we are calculating the attention map but I dont see where are we using it. A) When upsampling in the generator, we are also resizing them to match the feature map sizes.
- Understand how the filters, kernels and strides are configured to achieve different things (In our case, it is just a matter of doubling the number of filters compared to the previous layer and vice versa when upsampling.)
- How are we enforcing that the range of the generator's output is between [-1, 1]?
- Find out the following from Richard: Find out the following: Its usually Conv->Batch-> Relu but here its Conv-> Relu->Batch. Answer! MLM states that this is not an issue. Org. paper has it like normal but experimentation revealed that better perf.
is achieved if placed after the activation layer. If possible, try to test both!
- Why do many of the classes accept a parameter? An example being the unaligned_dataset class in unaligned_dataset.py
- In vgg_preprocess, why are the colour channels reversed?


My Experimentations:
- To upsample in the generator, try the transpose convolutional layer
- Check what happens if I remove the resize in the set_input function (single model.py)
- Its okay if our training results seem small because for the prediction process, the original size is maintained.
- Try to filter out line 265 in single_model.py. It seems to me that many aspects are redundant.
- Try another form of downsampling in the generator( to remove the maxpooling)
- Since I'm assuming that the model is being trained for even darkness, take matters into my own hands and try with my own images to verify!
-TRY TO PUT THE BATCH NORMALIZATION BEFORE THE LReLu(MLM says before but many forums found empirically that BN after LReLu performs better)
- See what happens if we dont decay the learning rate after 100 epochs
- For my experiment on batch vs instance normalization, there is a dedicated function (nn.InstanceNorm2d(in_features)--> Look into it more!)
-DO NOT MESS WITH THE DISCRIMINATOR, THE DISCRIMINATOR IS THE GOSPEL TRUTH (EGAN AND MSG have the same form of discriminator.)

- (Outcome Pending) It is what it is when I train one after another but tell them its the conflicting between the 2. Its not the architecture since most shadow removals use my architecture. Show when trained seperately and bring it to Hairong's attention. She said herself to train sequentially!
- I see that they are using maxpooling, I should see if I can instead use the transpose_conv to down sample(This move is backed by Radford)

Notes for the Report:
- For the report, I should definitely include a diagram of the architecture since she specifically asked for it ( but it needs to be indistinguishable from Enlighten-GAN!)
- (Outcome pending on this) In my report, I should show off that I dont need anything too dramatic for shadow detection (like ST-CGAN that dedicates an entire GAN for it)
- (Outcome pending) Find a way to do erosion and dilation to the attention map. Try doing these operations before adding to tensor but the noise removal will probably be expensive.
- When writing the report, describe how the data is pre-processed!
- In my write-up make sure I mention that Im using LSGAN loss and explain it. In the discriminator, instead of just being right or wrong, it tells how right or wrong we are.
- Find a way to remove the noise
- Be absolutely precise when it comes to detail, for example, when discussing the dataset, go into detail of where the images came from, what do they contain, what is it's form(png and size), why did I use it... Were they preprocessed
Make a note that I made this change : Check what is the best way to downsample--> EGAN: MaxPool.... I'm following Radford and doing fractionally strided convolution ( Give a thorough explain why this is the case)
- The generator and discriminator are DCGAN's but the grand network is considered as a LSGAN (the name primarily originating from the choice of loss function)
